{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part2_ModelDevelopmentAndPrediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Atmospheric correction of Sentinel 2 image using Py6S in Google Colab environment**\n",
        "\n",
        "This is the second part of python codes used in the article. The codes are tested inside Google Colab environment using Hong Kong water as the study area."
      ],
      "metadata": {
        "id": "bl3AKmzrS9Ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import required libraries & Initialize Google Earth Engine session**"
      ],
      "metadata": {
        "id": "5S6-DoMKTCdf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YTf5EwSJAbR"
      },
      "source": [
        "import ee\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import statsmodels.api as sm\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Match image data & in-situ station data"
      ],
      "metadata": {
        "id": "270h9bUtTImO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZtgGOLYdssE"
      },
      "source": [
        "# Load image data & in-situ station data\n",
        "assetList = ee.data.getList({'id':\"users/khoyinivan/S2_Py6S_mask_m\"})\n",
        "url = 'https://raw.githubusercontent.com/ivanhykwong/Marine-Water-Quality-Time-Series-HK/main/MarineQuality_2015-2020.csv'\n",
        "station_list = ['TM2','TM3','TM4','TM5','TM6','TM7','TM8','SM1','SM2','SM3','SM4','SM5','SM6','SM7','SM9','SM10','SM11',\n",
        "                'SM12','SM13','SM17','SM18','SM19','SM20','PM1','PM2','PM3','PM4','PM6','PM7','PM8','PM9','PM11','JM3',\n",
        "                'JM4','DM1','DM2','DM3','DM4','DM5','NM1','NM2','NM3','NM5','NM6','NM8','MM1','MM2','MM3','MM4','MM5',\n",
        "                'MM6','MM7','MM8','MM13','MM14','MM15','MM16','MM17','MM19','WM1','WM2','WM3','WM4','EM1','EM2','EM3',\n",
        "                'VM1','VM2','VM4','VM5','VM6','VM7','VM8','VM12','VM14','VM15']\n",
        "df_url = pd.read_csv(url)\n",
        "df_url = df_url[df_url['Station'].isin(station_list)]\n",
        "print(assetList)\n",
        "print(len(assetList))\n",
        "aoi = ee.Geometry.Polygon([[[113.800, 22.570],[113.800, 22.120],[114.514, 22.120],[114.514, 22.570]]])\n",
        "df_data = pd.DataFrame()\n",
        "\n",
        "for i in range(len(assetList)):\n",
        "  # Extract image date\n",
        "  assetid = assetList[i]['id']\n",
        "  print(assetid)\n",
        "  d1 = ee.Image(assetid)\n",
        "  d1_date = d1.date().format('yyyy-MM-dd')\n",
        "  print(d1_date.getInfo())\n",
        "\n",
        "  # Find nearest date between image & station data\n",
        "  df = df_url.copy()\n",
        "  df['Dates'] = pd.to_datetime(df['Dates'], format='%Y-%m-%d')\n",
        "  imagedate = datetime.strptime(d1_date.getInfo(), '%Y-%m-%d')\n",
        "  df['Image_date'] = imagedate\n",
        "  df['Date_compare'] = abs(df['Dates'] - imagedate)\n",
        "  df = df.sort_values(by=['Date_compare'])\n",
        "  df = df.drop_duplicates(subset=['Station'])\n",
        "  print(df.shape)\n",
        "\n",
        "  # Match image & station data, extract values to dataframe\n",
        "  pts = ee.FeatureCollection(\"users/khoyinivan/MonitoringStation_wgs84_76\")\n",
        "  pt_list = pts.toList(pts.size())\n",
        "  df[['B1','B2','B3','B4','B5','B6','B7','B8','B11','B12']] = np.nan\n",
        "  for pt in range(pt_list.length().getInfo()):\n",
        "    pt1 = ee.Feature(pt_list.get(pt))\n",
        "    pt1_buf = pt1.buffer(20)\n",
        "    s2_dict = d1.reduceRegion(ee.Reducer.mean(), pt1_buf.geometry()).getInfo()\n",
        "    n = pt1_buf.getInfo()['properties']['WaterStati']\n",
        "    for b in ['B1','B2','B3','B4','B5','B6','B7','B8','B11','B12']:\n",
        "      df.loc[df['Station'] == n, b] = s2_dict[b]\n",
        "  df = df.dropna(subset = ['B2'])\n",
        "  df['n'] = df.shape[0]\n",
        "\n",
        "  # Combine all image dates\n",
        "  df_data = pd.concat([df_data, df])\n",
        "\n",
        "# Export tables\n",
        "df_data.to_csv('df_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block is written for inputing csv file as the data, skip this block if the dataframe is already loaded"
      ],
      "metadata": {
        "id": "lCerveSYTTfy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu7U3QGzs3S5"
      },
      "source": [
        "# for start from reading file only, skip this block if from above\n",
        "df_data = pd.read_csv('df_data.csv')\n",
        "df_data['Image_date'] = pd.to_datetime(df_data['Image_date'], format='%Y-%m-%d')\n",
        "df_data['Date_compare'] = pd.to_timedelta(df_data['Date_compare'])\n",
        "df_data = df_data.drop(columns=['Unnamed: 0'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Extract observations & create variables"
      ],
      "metadata": {
        "id": "YUHga0c3T6QN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qM8M5UUWB_7"
      },
      "source": [
        "# Extract observations with <1 day difference\n",
        "\n",
        "max_day_diff = 1\n",
        "\n",
        "df = df_data[['Image_date', 'Dates', 'Date_compare', 'n', \n",
        "              '5-day Biochemical Oxygen Demand mg_L', 'Ammonia Nitrogen mg_L', 'Chlorophyll-a ug_L', 'Dissolved Oxygen mg_L',\n",
        "              'E. coli cfu_100mL', 'Faecal Coliforms cfu_100mL', 'Nitrate Nitrogen mg_L', 'Nitrite Nitrogen mg_L', \n",
        "              'Orthophosphate Phosphorus mg_L', 'pH', 'Salinity psu', 'Secchi Disc Depth M', 'Silica mg_L', \n",
        "              'Suspended Solids mg_L', 'Temperature C', 'Total Inorganic Nitrogen mg_L', 'Total Kjeldahl Nitrogen mg_L', \n",
        "              'Total Nitrogen mg_L', 'Total Phosphorus mg_L', 'Turbidity NTU', 'Unionised Ammonia mg_L', 'Volatile Suspended Solids mg_L',\n",
        "              'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12']].copy()\n",
        "\n",
        "df = df.rename(columns={'Image_date': 'Image_Date', 'Dates': 'Station_Date', \n",
        "                        '5-day Biochemical Oxygen Demand mg_L': 'BOD', 'Ammonia Nitrogen mg_L': 'AmNi', 'Chlorophyll-a ug_L': 'Chla', 'Dissolved Oxygen mg_L': 'DO',\n",
        "                        'E. coli cfu_100mL': 'Ecoli', 'Faecal Coliforms cfu_100mL': 'FC', 'Nitrate Nitrogen mg_L': 'NitraNi', 'Nitrite Nitrogen mg_L': 'NitriNi', \n",
        "                        'Orthophosphate Phosphorus mg_L': 'OrPh', 'pH': 'pH', 'Salinity psu': 'Sal', 'Secchi Disc Depth M': 'SDD', 'Silica mg_L': 'Si', \n",
        "                        'Suspended Solids mg_L': 'SS', 'Temperature C': 'Temp', 'Total Inorganic Nitrogen mg_L': 'TIN', 'Total Kjeldahl Nitrogen mg_L': 'TKN', \n",
        "                        'Total Nitrogen mg_L': 'ToNi', 'Total Phosphorus mg_L': 'ToPh', 'Turbidity NTU': 'Tur', 'Unionised Ammonia mg_L': 'UnAm', 'Volatile Suspended Solids mg_L': 'VSS'})\n",
        "\n",
        "df['Date_compare'] = pd.to_numeric(df['Date_compare'].dt.days)\n",
        "df['Image_Year'] = pd.DatetimeIndex(df['Image_Date']).year\n",
        "\n",
        "df = df[(df['Date_compare'] <= max_day_diff) & (df['n'] >= 10)].copy().drop(columns=['Image_Date', 'Station_Date', 'Date_compare', 'n'])\n",
        "\n",
        "# Remove outlier using Tukeyâ€™s fences method\n",
        "\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12']].any(axis=1)]\n",
        "\n",
        "# Replace 0 to min/2 (avoid errors in log operation)\n",
        "\n",
        "wq = ['BOD', 'AmNi', 'Chla', 'DO', 'Ecoli', 'FC', 'NitraNi', 'NitriNi', 'OrPh', 'pH', 'Sal', 'SDD', 'Si', 'SS', 'Temp', 'TIN', 'TKN', 'ToNi', 'ToPh', 'Tur', 'UnAm', 'VSS']\n",
        "for a in wq:\n",
        "  df[a]=df[a].replace(0, np.NaN)\n",
        "  df[a]=df[a].replace(np.NaN,df[a].min()/2)\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sldxw9LW4fSD"
      },
      "source": [
        "# Create independent variables\n",
        "\n",
        "bands = ['B' + str(b) for b in [*range(1,9),11,12]]\n",
        "wl = [443,490,560,665,705,740,783,842,1610,2190]\n",
        "\n",
        "# Two-band ratio\n",
        "for i in bands:\n",
        "  for j in bands:\n",
        "    if i != j:\n",
        "      df['BR_'+i+j] = df[i] / df[j]\n",
        "      # B_ratio.append('B'+str(i)+'_B'+str(j))\n",
        "\n",
        "# Three-band ratio\n",
        "for i in range(0,10):\n",
        "  for j in range(0,10):\n",
        "    for k in range(0,10):\n",
        "      if (j == i+1) & (k == j+1):\n",
        "        df['TB_'+bands[i]+bands[j]+bands[k]] = ((1/df[bands[i]]) - (1/df[bands[j]])) * df[bands[k]]\n",
        "\n",
        "# Line height algorithm\n",
        "for i in range(0,10):\n",
        "  for j in range(0,10):\n",
        "    for k in range(0,10):\n",
        "      if (j == i+1) & (k == j+1):\n",
        "        df['LH_'+bands[i]+bands[j]+bands[k]] = df[bands[j]] - df[bands[i]] - ((df[bands[k]] - df[bands[i]]) * ((wl[j]-wl[i])/(wl[k]-wl[i])))\n",
        "\n",
        "df.to_csv('df_data_filter.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Stepwise regression"
      ],
      "metadata": {
        "id": "7LkflG_dUMZQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAn292ai2p9W"
      },
      "source": [
        "# Define functions for stepwise regression \n",
        "# Modified from https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn/24447#24447\n",
        "\n",
        "def stepwise_selection(X, y, \n",
        "                       initial_list=[], \n",
        "                       threshold_in=0.05, \n",
        "                       threshold_out = 0.1, \n",
        "                       verbose=True):\n",
        "    \"\"\" Perform a forward-backward feature selection \n",
        "    based on p-value from statsmodels.api.OLS\n",
        "    Arguments:\n",
        "        X - pandas.DataFrame with candidate features\n",
        "        y - pandas.DataFrame with the target column\n",
        "        initial_list - list of features to start with (column names of X)\n",
        "        threshold_in - include a feature if its p-value < threshold_in\n",
        "        threshold_out - exclude a feature if its p-value > threshold_out\n",
        "        verbose - whether to print the sequence of inclusions and exclusions\n",
        "    Returns: list of selected features \n",
        "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
        "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
        "    \"\"\"\n",
        "    y = y.to_numpy()\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed=False\n",
        "        # forward step\n",
        "        excluded = list(set(X.columns)-set(included))\n",
        "        new_pval = pd.Series(index=excluded, dtype='float64')\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        best_pval = new_pval.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_feature = new_pval.index[new_pval.argmin()]\n",
        "            included.append(best_feature)\n",
        "            changed=True\n",
        "            if verbose:\n",
        "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
        "\n",
        "        # backward step\n",
        "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
        "        # use all coefs except intercept\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max() # null if pvalues is empty\n",
        "        if worst_pval > threshold_out:\n",
        "            changed=True\n",
        "            worst_feature = pvalues.index[pvalues.argmax()]\n",
        "            included.remove(worst_feature)\n",
        "            if verbose:\n",
        "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
        "        if not changed:\n",
        "            break\n",
        "    return included\n",
        "\n",
        "def fit_regression(df, X_all, Y_name):\n",
        "  Y_ln = np.log(df[Y_name])\n",
        "  \n",
        "  stepwise_variables = stepwise_selection(X_all, Y_ln, verbose=False)\n",
        "  # print('resulting features:')\n",
        "  # print(stepwise_variables)\n",
        "\n",
        "  X = X_all[stepwise_variables]\n",
        "  regr = linear_model.LinearRegression()\n",
        "  regr.fit(X, Y_ln)\n",
        "  r_squared = regr.score(X, Y_ln)\n",
        "  adjusted_r_squared = 1 - (1-r_squared)*(len(Y_ln)-1)/(len(Y_ln)-X.shape[1]-1)\n",
        "\n",
        "  print(Y_name + ': ' + str(r_squared))\n",
        "\n",
        "  # Evaluate model\n",
        "  Y = df[Y_name]\n",
        "  Y_pred = np.exp(regr.predict(X))\n",
        "  corr_orig = np.corrcoef(Y, Y_pred)[0, 1]\n",
        "  regr_eval = linear_model.LinearRegression()\n",
        "  regr_eval.fit(Y_pred.reshape(-1, 1), Y)\n",
        "  r_squared_orig = regr_eval.score(Y_pred.reshape(-1, 1), Y)\n",
        "  adjusted_r_squared_orig = 1 - (1-r_squared_orig)*(len(Y)-1)/(len(Y)-X.shape[1]-1)\n",
        "  rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
        "  rmspe = np.sqrt(np.mean(np.square(((Y - Y_pred) / Y)), axis=0))\n",
        "  mae = mean_absolute_error(Y, Y_pred)\n",
        "  mape = np.mean(np.abs(((Y - Y_pred) / Y)))\n",
        "\n",
        "  regression_df = pd.DataFrame({'Y': [Y_name], 'r2': [r_squared], 'adjusted_r2': [adjusted_r_squared],\n",
        "                                'corr_orig': [corr_orig], 'r2_orig': [r_squared_orig], 'adjusted_r2_orig': [adjusted_r_squared_orig],\n",
        "                                'rmse': [rmse], 'rmspe': [rmspe], 'mae': [mae], 'mape': [mape], \n",
        "                                'X': [stepwise_variables], 'intercept': [regr.intercept_], 'coef': [regr.coef_]})\n",
        "  predicted_df = pd.DataFrame({'Y_ln': Y_ln, 'Pred_ln': regr.predict(X), 'Y': Y, 'Pred': Y_pred})\n",
        "\n",
        "  return regression_df, predicted_df\n",
        "\n",
        "def test_regression(df_test, X_test, Y_name, regression_df):\n",
        "\n",
        "  regr_int = regression_df['intercept'][0]\n",
        "  regr_variables = regression_df['X'][0]\n",
        "  regr_coef = regression_df['coef'][0]\n",
        "\n",
        "  X_test['Y_pred'] = regr_int\n",
        "  for n in range(len(regr_variables)):\n",
        "    X_test['Y_pred'] = X_test['Y_pred'] + X_test[regr_variables[n]]*regr_coef[n]\n",
        "\n",
        "  Y = df_test[Y_name]\n",
        "  Y_ln = np.log(df_test[Y_name])\n",
        "  Y_pred_ln = X_test['Y_pred']\n",
        "  Y_pred = np.exp(X_test['Y_pred'])\n",
        "  \n",
        "  # Evaluate model\n",
        "  regr = linear_model.LinearRegression()\n",
        "  regr.fit(Y.values.reshape(-1,1), Y_pred.values.reshape(-1,1))\n",
        "  r_squared = regr.score(Y.values.reshape(-1,1), Y_pred.values.reshape(-1,1))\n",
        "  corr = np.corrcoef(Y, Y_pred)[0, 1]\n",
        "  rmse = mean_squared_error(Y, Y_pred, squared=False)\n",
        "  rmspe = np.sqrt(np.mean(np.square(((Y - Y_pred) / Y)), axis=0))\n",
        "  mae = mean_absolute_error(Y, Y_pred)\n",
        "  mape = np.mean(np.abs(((Y - Y_pred) / Y)))\n",
        "\n",
        "  regr_ln = linear_model.LinearRegression()\n",
        "  regr_ln.fit(Y_ln.values.reshape(-1,1), Y_pred_ln.values.reshape(-1,1))\n",
        "  r_squared_ln = regr_ln.score(Y_ln.values.reshape(-1,1), Y_pred_ln.values.reshape(-1,1))\n",
        "  corr_ln = np.corrcoef(Y_ln, Y_pred_ln)[0, 1]\n",
        "\n",
        "  test_summary_df = pd.DataFrame({'Y': [Y_name], 'r2': [r_squared], 'r2_ln': [r_squared_ln],\n",
        "                                'corr': [corr], 'corr_ln': [corr_ln], \n",
        "                                'rmse': [rmse], 'rmspe': [rmspe], 'mae': [mae], 'mape': [mape]})\n",
        "  test_data_df = pd.DataFrame({'Y_ln': Y_ln, 'Y_pred_ln': Y_pred_ln, 'Y': Y, 'Y_pred': Y_pred})\n",
        "\n",
        "  return test_summary_df, test_data_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block is written for inputing csv file as the data, skip this block if the dataframe is already loaded"
      ],
      "metadata": {
        "id": "ZIBT5XBwUVh5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MxIbH4UofAj"
      },
      "source": [
        "# for start from reading file\n",
        "df = pd.read_csv('df_data_filter.csv')\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgPce6KNDluK"
      },
      "source": [
        "# Seperate into training/testing sets\n",
        "\n",
        "df_train = df[df['Image_Year'] <= 2019].drop(columns=['Image_Year']).copy()\n",
        "df_test = df[df['Image_Year'] == 2020].drop(columns=['Image_Year']).copy()\n",
        "X_train = df_train.drop(columns = wq)\n",
        "X_test = df_test.drop(columns = wq)\n",
        "\n",
        "# Perform stepwise regression (model development)\n",
        "\n",
        "train_regr_result_list, train_pred_result_list = map(list,zip(*[fit_regression(df_train, X_train, value) for value in wq]))\n",
        "train_regr_result = pd.concat(train_regr_result_list)\n",
        "train_pred_result = pd.concat(train_pred_result_list, axis=1)\n",
        "wq_col_list = [[value+'_ln', value+'Pred_ln', value, value+'Pred'] for value in wq]\n",
        "wq_col = [item for sublist in wq_col_list for item in sublist]\n",
        "train_pred_result.columns = wq_col\n",
        "train_regr_result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK8rOhQMCXQf"
      },
      "source": [
        "# Perform stepwise regression (validation)\n",
        "\n",
        "test_regr_summary_list, test_regr_data_list = map(list,zip(*[test_regression(df_test, X_test, value, train_regr_result[train_regr_result['Y']==value]) for value in wq]))\n",
        "\n",
        "test_regr_summary = pd.concat(test_regr_summary_list)\n",
        "test_regr_data = pd.concat(test_regr_data_list, axis=1)\n",
        "test_regr_data.columns = wq_col\n",
        "test_regr_summary\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFY7UYqJIq9i"
      },
      "source": [
        "# Save results\n",
        "\n",
        "train_regr_result.to_csv('train_regr_result.csv')\n",
        "train_pred_result.to_csv('train_pred_result.csv')\n",
        "test_regr_summary.to_csv('test_regr_summary.csv')\n",
        "test_regr_data.to_csv('test_regr_data.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Apply regression models"
      ],
      "metadata": {
        "id": "Mn33CDTqUhqE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uU2nHFJvUck"
      },
      "source": [
        "# Apply regression models to images (entire image collection)\n",
        "\n",
        "assetList = ee.data.getList({'id':\"users/khoyinivan/S2_Py6S_mask_m\"})\n",
        "print(assetList)\n",
        "print(len(assetList))\n",
        "aoi = ee.Geometry.Polygon([[[113.800, 22.570],[113.800, 22.120],[114.514, 22.120],[114.514, 22.570]]])\n",
        "\n",
        "for i in range(len(assetList)):\n",
        "  assetid = assetList[i]['id']\n",
        "  print(assetid)\n",
        "  d1 = ee.Image(assetid)\n",
        "  d1_date = d1.date().format('yyyy-MM-dd')\n",
        "  print(d1_date.getInfo())\n",
        "  imagedate = datetime.strptime(d1_date.getInfo(), '%Y-%m-%d')\n",
        "\n",
        "  # Chla\n",
        "  name = ('Chla' + d1_date.getInfo()).replace('-','')\n",
        "  regr_int = 0.66220\n",
        "  # Variable: 'LH_B1B2B3', 'B1', 'B5', 'LH_B7B8B11', 'LH_B8B11B12', 'BR_B3B2', 'BR_B12B3', 'LH_B2B3B4'\n",
        "  regr_coef = [-137.59036, -14.78696, 15.69912, 55.41826, -25.37227, 1.18410, -0.42734, -18.73902]\n",
        "  d1_predict = d1.expression(\n",
        "      'Int + C0*(B2-B1-(B3-B1)*((490-443)/(560-443))) + C1*B1 + C2*B5 + C3*(B8-B7-(B11-B7)*((842-783)/(1610-783))) + C4*(B11-B8-(B12-B8)*((1610-842)/(2190-842))) + C5*(B3/B2) + C6*(B12/B3) + C7*(B3-B2-(B4-B2)*((560-490)/(665-490)))', {\n",
        "          'Int': regr_int,\n",
        "          'C0': regr_coef[0], 'C1': regr_coef[1], 'C2': regr_coef[2], 'C3': regr_coef[3],\n",
        "          'C4': regr_coef[4], 'C5': regr_coef[5], 'C6': regr_coef[6], 'C7': regr_coef[7],\n",
        "          'B1': d1.select('B1'), 'B2': d1.select('B2'), 'B3': d1.select('B3'), 'B4': d1.select('B4'), 'B5': d1.select('B5'), \n",
        "          'B7': d1.select('B7'), 'B8': d1.select('B8'), 'B11': d1.select('B11'), 'B12': d1.select('B12')\n",
        "          }).exp().rename(name).set('system:time_start', ee.Date(d1_date).millis())\n",
        "\n",
        "  task = ee.batch.Export.image.toAsset(image=d1_predict, description=name, assetId = 'users/khoyinivan/S2_Chla/' + name, scale = 10, region = aoi)\n",
        "  task.start()\n",
        "\n",
        "  # SS\n",
        "  name = ('SS' + d1_date.getInfo()).replace('-','')\n",
        "  regr_int = 0.80715\n",
        "  # Variable: 'B3', 'LH_B4B5B6', 'LH_B8B11B12', 'BR_B12B3', 'LH_B7B8B11', 'B2', 'BR_B8B3', 'BR_B7B2', 'BR_B11B2'\n",
        "  regr_coef = [38.11636, 56.64875, -54.68850, 1.03707, -49.44436, -30.55160, 1.80722, -1.32230, -0.93927]\n",
        "  d1_predict = d1.expression(\n",
        "      'Int + C0*B3 + C1*(B5-B4-(B6-B4)*((705-665)/(740-665))) + C2*(B11-B8-(B12-B8)*((1610-842)/(2190-842))) + C3*(B12/B3) + C4*(B8-B7-(B11-B7)*((842-783)/(1610-783))) + C5*B2 + C6*(B8/B3) + C7*(B7/B2) + C8*(B11/B2)', {\n",
        "          'Int': regr_int,\n",
        "          'C0': regr_coef[0], 'C1': regr_coef[1], 'C2': regr_coef[2], 'C3': regr_coef[3],\n",
        "          'C4': regr_coef[4], 'C5': regr_coef[5], 'C6': regr_coef[6], 'C7': regr_coef[7], 'C8': regr_coef[8], \n",
        "          'B2': d1.select('B2'), 'B3': d1.select('B3'), 'B4': d1.select('B4'), 'B5': d1.select('B5'), 'B6': d1.select('B6'), \n",
        "          'B7': d1.select('B7'), 'B8': d1.select('B8'), 'B11': d1.select('B11'), 'B12': d1.select('B12')\n",
        "          }).exp().rename(name).set('system:time_start', ee.Date(d1_date).millis())\n",
        "\n",
        "  task = ee.batch.Export.image.toAsset(image=d1_predict, description=name, assetId = 'users/khoyinivan/S2_SS/' + name, scale = 10, region = aoi)\n",
        "  task.start()\n",
        "\n",
        "  # Tur\n",
        "  name = ('Tur' + d1_date.getInfo()).replace('-','')\n",
        "  regr_int = 2.67180\n",
        "  # Variable: 'BR_B3B4', 'LH_B2B3B4', 'BR_B8B2', 'BR_B11B7', 'BR_B1B3', 'BR_B6B2', 'BR_B3B2', 'LH_B4B5B6', 'LH_B3B4B5'\n",
        "  regr_coef = [0.00528, 64.73459, -2.00839, -0.00082, -0.99431, 2.11603, -1.54066, 119.18667, 32.45938]\n",
        "  d1_predict = d1.expression(\n",
        "      'Int + C0*(B3/B4) + C1*(B3-B2-(B4-B2)*((560-490)/(665-490))) + C2*(B8/B2) + C3*(B11/B7) + C4*(B1/B3) + C5*(B6/B2) + C6*(B3/B2) + C7*(B5-B4-(B6-B4)*((705-665)/(740-665))) + C8*(B4-B3-(B5-B3)*((665-560)/(705-560)))', {\n",
        "          'Int': regr_int,\n",
        "          'C0': regr_coef[0], 'C1': regr_coef[1], 'C2': regr_coef[2], 'C3': regr_coef[3],\n",
        "          'C4': regr_coef[4], 'C5': regr_coef[5], 'C6': regr_coef[6], 'C7': regr_coef[7], 'C8': regr_coef[8], \n",
        "          'B1': d1.select('B1'), 'B2': d1.select('B2'), 'B3': d1.select('B3'), 'B4': d1.select('B4'), 'B5': d1.select('B5'), 'B6': d1.select('B6'), \n",
        "          'B7': d1.select('B7'), 'B8': d1.select('B8'), 'B11': d1.select('B11')\n",
        "          }).exp().rename(name).set('system:time_start', ee.Date(d1_date).millis())\n",
        "\n",
        "  task = ee.batch.Export.image.toAsset(image=d1_predict, description=name, assetId = 'users/khoyinivan/S2_Tur/' + name, scale = 10, region = aoi)\n",
        "  task.start()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}